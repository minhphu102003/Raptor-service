# RAPTOR Service ‚Äì Build Pipeline Report (Upload ‚Üí Tree Build)

> **Scope:** T√†i li·ªáu m√¥ t·∫£ _end-to-end_ pipeline c·ªßa **Raptor-service** t·ª´ l√∫c **upload d·ªØ li·ªáu** ƒë·∫øn khi **x√¢y c√¢y RAPTOR** (tree building). Bao g·ªìm ki·∫øn tr√∫c, API, tham s·ªë, ki·ªÉm th·ª≠ nhanh, v√† s∆° ƒë·ªì tr·ª±c quan (Mermaid).

---

## 0) B·ªëi c·∫£nh & M·ª•c ti√™u

- **RAPTOR** (Recursive Abstractive Processing for Tree-Organized Retrieval) x√¢y c√¢y t√≥m t·∫Øt ph√¢n c·∫•p b·∫±ng c√°ch **embed ‚Üí gi·∫£m chi·ªÅu (UMAP) ‚Üí ph√¢n c·ª•m (GMM+BIC) ‚Üí t√≥m t·∫Øt b·∫±ng LLM ‚Üí re-embed**, l·∫∑p l·∫°i theo t·∫ßng cho t·ªõi khi d·ª´ng.
- D·ªãch v·ª• **Raptor-service** cung c·∫•p API ƒë·ªôc l·∫≠p ƒë·ªÉ **nh·∫≠n c√°c chunk** (c√≥ th·ªÉ ƒë√£ embed) v√† **x√¢y tree**; k√®m retrieval sau n√†y.

> Tham chi·∫øu: ph∆∞∆°ng ph√°p RAPTOR (arXiv 2401.18059) v√† c·∫•u tr√∫c repo + I/O spec c·ªßa d·ª± √°n.

---

## 1) Ki·∫øn tr√∫c c·∫•p cao (High-level)

**Th√†nh ph·∫ßn logic ch√≠nh** (theo c·∫•u tr√∫c th∆∞ m·ª•c):

- `app/` ‚Äì **FastAPI**: kh·ªüi t·∫°o app, container DI, settings, include routers.
- `services/` ‚Äì lu·ªìng nghi·ªáp v·ª•: `build_tree_service.py` (x√¢y c√¢y)
- `infra/` ‚Äì h·∫° t·∫ßng: `embeddings/` (backend t·∫°o vector), `llm/` (g·ªçi model t√≥m t·∫Øt), `db/` (ORM/Unit of Work), `uow/`.
- `interfaces_adaptor/` ‚Äì adapter: HTTP, gateways, repository, client.
- `tests/` ‚Äì test API (build/retrieve), factories payload.
- `alembic/` + `Database.md` ‚Äì l∆∞·ª£c ƒë·ªì DB & migration.
- `utils` - c√°c h√†m ti·ªán √≠ch d√πng chung
- `logs` - th∆∞ m·ª•c ch·ª©a log d√πng ƒë·ªÉ trace v√† debug
- `constants` - th∆∞ m·ª•c ch·ª©a c√°c bi·∫øn c·ªë ƒë·ªãnh nh∆∞ prompt

### 1.1 S∆° ƒë·ªì d√≤ng d·ªØ li·ªáu (upload ‚Üí build)

``mermaid
flowchart TD
%% Gate: only .md
A["Client Upload<br/>(.md or other)"] --> B{"Is Markdown (.md)?"}
B -- "No" --> R["Return error/unsupported<br/>(only .md accepted)"]
B -- "Yes" --> P["Start .md pipeline"]

    %% Start two tracks: Save doc and Pre-chunk in memory
    P --> D1["Save original document<br/>(blob + metadata)<br/>(returns doc_id)"]
    P --> T0["Start chunking pipeline"]

    %% Chunking & refinement (DeepSeek V3 @ temp=0)
    subgraph CH ["Chunking & Refinement"]
      direction TB
      T0 --> T1["Naive Markdown chunking<br/>(RecursiveCharacterTextSplitter)"]
      T1 --> T2["LLM-guided refinement<br/>(DeepSeek-V3, temperature = 0)"]
      T2 --> T3["Prepare chunk texts[]"]
    end

    %% Join barrier: must have doc_id before persisting chunks/embeddings
    D1 --> J["JOIN: doc_id available"]
    T3 --> J
    J --> C3["Persist chunks to Supabase<br/>(chunks table; doc_id FK)"]

    %% Embedding strictly after chunking
    C3 --> E1["Contextualized embeddings (Voyage)<br/>model: voyage-context-3; output_dimension = 1024"]
    E1 --> E2["Persist embeddings to Supabase<br/>(embeddings table; vector(1024))"]

    %% Rebuild decision by database_id (before RAPTOR build)
    E2 --> Q0["Lookup by database_id<br/>(documents/chunks/embeddings & existing tree)"]
    Q0 -- "None found" --> B0["Build set = current chunks + embeddings"]

    %% PARALLEL: delete old tree(s) and fetch previous chunks+embeddings
    Q0 -- "Found" --> PAR_FORK["Fork: cleanup & load in parallel"]
    subgraph PAR ["Parallel cleanup & load"]
      direction LR
      PAR_FORK --> TDEL["Delete old tree(s)<br/>(for database_id)"]
      PAR_FORK --> FOLD["Fetch previous chunks + embeddings<br/>(by database_id)"]
      TDEL --> J2["Join: both complete"]
      FOLD --> J2
    end

    %% Build set after join
    J2 --> MRG["Merge: previous + current ‚Üí build set"]

    %% Continue RAPTOR build with the chosen build set
    B0 --> V["Validate embeddings<br/>(dim = 1024; choose distance op)"]
    MRG --> V
    V --> L0["Init: level = 0<br/>current_ids/vecs/texts = build set"]

subgraph RL["RAPTOR level loop"]
direction TB
L0 --> CL["Cluster (includes UMAP) + GMM<br/>(fit_predict: min_k..max_k; criterion = BIC)<br/>logs: clusters, sizes, best_score"]
CL --> S["Summarize groups<br/>(_summarize_groups; async via sem)"]
S --> E["Embed summaries<br/>(_embed_with_throttle; min_interval)"]
E --> AH["Persist level<br/>(_persist_level: commit nodes & edges)<br/>update current_ids/vecs/texts; level = level + 1"]
AH --> CHK{"len(current_ids) > 1 ?"}
end

CHK -- "Yes ‚Üí next level" --> CL
CHK -- "No ‚Üí done" --> OUT["Finish build<br/>(persist final tree & index)"]

```

### 1.2 S∆° ƒë·ªì d√≤ng truy v·∫•n

``mermaid
flowchart TD
  A["POST /v1/retrieve (dataset_id, query, mode, top_k, expand_k, levels_cap, use_reranker, byok_key)"] --> N["Normalize + sanitize query"]
  N --> E["Embed query (Voyage context-3, dim=1024)"]
  E --> M{"mode ?"}

  %% Collapsed
  M --|collapsed|--> C0["Select candidate nodes across levels<br/>(respect levels_cap; include leaves+summaries)"]
  C0 --> C1["Vector search via pgvector (cosine_distance)<br/>top_k per chosen level(s)"]
  C1 --> C2["If a hit is a summary node ‚Üí expand children<br/>up to expand_k or until leaf"]
  C2 --> C3["Aggregate & deduplicate by chunk_id/doc_id"]
  C3 --> C4["(Optional) Rerank cross-encoder / API<br/>use_reranker=True ‚Üí reorder"]
  C4 --> R["Return contexts[] with scores + metadata"]

  %% Traversal
  M --|traversal|--> T0["Pick root/community nodes (highest level)"]
  T0 --> T1["Vector match on roots ‚Üí top_k_roots"]
  T1 --> T2["Beam search down the tree<br/>per level keep expand_k children by sim"]
  T2 --> T3{"Reached levels_cap or leaves?"}
  T3 --|No|--> T2
  T3 --|Yes|--> T4["Collect leaf chunks (+optionally parent summaries)"]
  T4 --> T5["(Optional) Rerank"]
  T5 --> R

  %% Answering (separate endpoint)
  R --> A0["POST /v1/answer"]
  A0 --> A1["Context window packing (budget by max_tokens)"]
  A1 --> A2["LLM (DeepSeek-V3 / GPT-4o-mini / Claude-Haiku / Gemini-Flash)<br/>temperature, stream"]
  A2 --> A3["Return answer + citations (chunk_ids)"]
```

### 1.3 S∆° ƒë·ªì tr√¨nh t·ª± (Async build)

``mermaid
sequenceDiagram
autonumber
participant C as Client
participant A as API (FastAPI)
participant DB as Supabase (Postgres/pgvector)
participant LLM as DeepSeek-V3 (chunk refine)
participant EMB as Voyage (contextual embeddings)
participant GDQ as Gemini<br>DeepSeek-V3<br>Qwen3-235B-A22B<br> (LLM summarize)

C->>A: POST /v1/document/ingest-markdown (database_id, file=.md)
A->>A: Validate request & file-type

    alt Not Markdown
      A-->>C: 400 Unsupported (only .md)
    else Is Markdown
      A->>DB: INSERT documents (blob + metadata)
      DB-->>A: doc_id
      Note over A,DB: JOIN barrier satisfied (have doc_id)

      A->>A: Start chunking pipeline (in-memory)
      A->>A: Naive MD chunking (size=1200, overlap=200)
      A->>LLM: Refine chunks (temperature=0)
      LLM-->>A: refined chunk texts[]

      A->>DB: INSERT chunks (doc_id, idx, text, meta)
      DB-->>A: chunk_ids[]
      A->>EMB: Embed chunks (voyage-context-3, dim=1024)
      EMB-->>A: embeddings[]
      A->>DB: INSERT embeddings (owner=chunk, vector(1024))

      A->>DB: Lookup existing tree(s) by database_id
      alt Found existing tree(s)
        par Delete old trees
          A->>DB: DELETE trees / tree_nodes / tree_edges
          DB-->>A: OK
        and Fetch previous data
          A->>DB: SELECT prev chunks + embeddings
          DB-->>A: prev_chunks[], prev_emb[]
        end
        A->>A: Merge previous + current -> build_set
      else None found
        A->>A: build_set = current chunks + embeddings
      end

      loop while more than one id (and under cap)
        A->>A: Cluster (UMAP inside) + GMM (BIC)
        A->>GDQ: Summarize groups
        GDQ-->>A: summaries[]
        A->>EMB: Embed summaries (throttle)
        EMB-->>A: summary vectors[]
        A->>DB: Persist tree_nodes and tree_edges
        A->>A: Update current ids/vecs/texts and increment level
      end

      A->>DB: Persist final tree (root, params, stats)
      A-->>C: 200 OK
    end

```

```mermaid
sequenceDiagram
  autonumber
  participant C as Client
  participant A as API (FastAPI)
  participant DB as Supabase (Postgres/pgvector)
  participant EMB as Voyage embeddings
  participant RER as Reranker (optional)

  C->>A: POST /v1/retrieve (dataset_id, query, mode, top_k, expand_k, levels_cap, use_reranker, reranker_model, byok_voyage_api_key)
  A->>A: Normalize query and defaults
  A->>DB: SELECT max(level) FROM raptor_nodes WHERE dataset_id = ...
  DB-->>A: max_level
  A->>EMB: Embed query (voyage-context-3, dim 1024)
  EMB-->>A: qvec (float[])

  alt mode == collapsed
    A->>DB: Vector search raptor_nodes at levels [0..cap]
    DB-->>A: hits (leaf + summary) with distance
    A->>A: Convert to similarity = 1 - distance
    A->>A: Attach score
    A->>DB: For each summary, fetch leaves (expand_k per parent)
    DB-->>A: expanded_leaf_candidates
    A->>A: Aggregate, deduplicate, sort by score desc
  else mode == traversal
    A->>DB: Search roots at level = cap
    DB-->>A: roots (frontier)
    loop level = cap-1 down to 0
      A->>DB: Children for frontier (expand_k per parent)
      DB-->>A: children with distance
      A->>A: Convert to similarity and keep beam size = top_k
      A->>A: Collect leaves
    end
    A->>A: If no leaves, fallback to top_k frontier
  end

  opt use_reranker == true
    A->>RER: Rerank(query, candidates, model)
    RER-->>A: order
    A->>A: Reorder candidates
  end

  A-->>C: 200 OK (results: id, doc_id, level, is_leaf, text, score, ...)

```

---

## 2) API ‚Äì h·ª£p ƒë·ªìng I/O

### 2.1 Build c√¢y ‚Äì `POST /v1/document/ingest-markdown`

**Content-Type:** `multipart/form-data`

**M√¥ t·∫£:** Upload **Markdown (.md)** k√®m metadata d·∫°ng form. H·ªá th·ªëng s·∫Ω:

1. L∆∞u document ƒë·ªÉ l·∫•y `doc_id` (JOIN barrier)
2. Chunk ‚Üí refine (LLM) ‚Üí embed (Voyage 1024d) ‚Üí index
3. (N·∫øu `build_tree=true`) build RAPTOR tree v√† tr·∫£ `tree_id`

> Y√™u c·∫ßu: c√†i `python-multipart` ƒë·ªÉ FastAPI nh·∫≠n form + file.

---

## Headers (tu·ª≥ ch·ªçn)

- `X-Dataset-Id`: ID dataset t·ª´ header (backend c√≥ th·ªÉ cho ph√©p ghi ƒë√®/∆∞u ti√™n theo ch√≠nh s√°ch).

---

## Form fields

| Tr∆∞·ªùng         | Ki·ªÉu                                         | B·∫Øt bu·ªôc | M·∫∑c ƒë·ªãnh   | Ghi ch√∫                             |
| -------------- | -------------------------------------------- | -------: | ---------- | ----------------------------------- |
| `file`         | `UploadFile` (.md)                           |       ‚úîÔ∏é | ‚Äì          | Ch·ªâ nh·∫≠n **Markdown**               |
| `dataset_id`   | `string`                                     |       ‚úîÔ∏é | ‚Äì          | ID b·ªô d·ªØ li·ªáu                       |
| `source`       | `string`                                     |       ‚úñÔ∏é | ‚Äì          | Ngu·ªìn g·ªëc t√†i li·ªáu (URL, path, ‚Ä¶)   |
| `tags`         | `string[]`                                   |       ‚úñÔ∏é | ‚Äì          | G·ª≠i l·∫∑p nhi·ªÅu field `tags` n·∫øu c·∫ßn  |
| `extra_meta`   | `string` (JSON)                              |       ‚úñÔ∏é | ‚Äì          | JSON encode, vd: `{"author":"..."}` |
| `build_tree`   | `bool`                                       |       ‚úñÔ∏é | `true`     | `true` ‚Üí build RAPTOR tree          |
| `summary_llm`  | `string`/enum                                |       ‚úñÔ∏é | ‚Äì          | Model t√≥m t·∫Øt (vd: `deepseek_v3`)   |
| `vector_index` | `string`                                     |       ‚úñÔ∏é | ‚Äì          | T√™n/kho√° c·∫•u h√¨nh index vector      |
| `upsert_mode`  | `"upsert" \| "replace" \| "skip_duplicates"` |       ‚úñÔ∏é | `"upsert"` | Chi·∫øn l∆∞·ª£c ghi d·ªØ                   |

---

## V√≠ d·ª• cURL

```
curl -X POST "$HOST/v1/document/ingest-markdown"   -H "X-Dataset-Id: ds_demo"   -H "Accept: application/json"   -F "dataset_id=ds_demo"   -F "file=@/path/to/readme.md;type=text/markdown"   -F "source=https://example.com/readme.md"   -F "tags=docs" -F "tags=markdown"   -F 'extra_meta={"category":"guide"}'   -F "build_tree=true"   -F "summary_llm=deepseek_v3"   -F "vector_index=hnsw_cosine"   -F "upsert_mode=upsert"   -F "byok_voyage_api_key=****"
```

---

## Response (200 OK)

```json
{
  "code": 200,
  "data": {
    "doc_id": "d216eea618c84093baae2ec68961f35a",
    "dataset_id": "ds_demo",
    "status": "embedded",
    "chunks": 81,
    "indexed": {
      "upserted": 81
    },
    "tree_id": "d216eea618c84093baae2ec68961f35a::tree",
    "checksum": "0ca1fa7fa265142bb573aa7a99926f4c8430748ffe9f888cde8cb81e93b30551"
  }
}
```

### M√£ l·ªói ph·ªï bi·∫øn

- `400 BAD_REQUEST` ‚Äì File kh√¥ng ph·∫£i `.md`, thi·∫øu `dataset_id`, ho·∫∑c form kh√¥ng h·ª£p l·ªá.
- `415 UNSUPPORTED_MEDIA_TYPE` ‚Äì Kh√¥ng g·ª≠i `multipart/form-data`.
- `422 UNPROCESSABLE_ENTITY` ‚Äì Sai ki·ªÉu d·ªØ li·ªáu form/header theo schema FastAPI.
- `500 INTERNAL` ‚Äì L·ªói n·ªôi b·ªô khi chunking/embedding/indexing.

---

### 2.2 Truy xu·∫•t d·ªØ li·ªáu ‚Äì `POST /v1/document/retrieve`

**Content-Type:** `application/json`

**M√¥ t·∫£:** Truy xu·∫•t c√°c ƒëo·∫°n vƒÉn b·∫£n li√™n quan ƒë·∫øn truy v·∫•n t·ª´ c∆° s·ªü tri th·ª©c.

---

## Request Body

| Tr∆∞·ªùng                | Ki·ªÉu     | B·∫Øt bu·ªôc | M·∫∑c ƒë·ªãnh      | Ghi ch√∫                                        |
| --------------------- | -------- | -------: | ------------- | ---------------------------------------------- |
| `dataset_id`          | `string` |       ‚úîÔ∏é | ‚Äì             | ID b·ªô d·ªØ li·ªáu                                  |
| `query`               | `string` |       ‚úîÔ∏é | ‚Äì             | Truy v·∫•n t√¨m ki·∫øm                              |
| `mode`                | `string` |       ‚úñÔ∏é | `"collapsed"` | `"collapsed"` ho·∫∑c `"traversal"`               |
| `top_k`               | `int`    |       ‚úñÔ∏é | `8`           | S·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn b·∫£n tr·∫£ v·ªÅ                   |
| `expand_k`            | `int`    |       ‚úñÔ∏é | `5`           | S·ªë l∆∞·ª£ng node m·ªü r·ªông (ch·ªâ v·ªõi mode collapsed) |
| `levels_cap`          | `int`    |       ‚úñÔ∏é | `0`           | Gi·ªõi h·∫°n c·∫•p ƒë·ªô c√¢y (ch·ªâ v·ªõi mode traversal)   |
| `use_reranker`        | `bool`   |       ‚úñÔ∏é | `false`       | C√≥ s·ª≠ d·ª•ng reranker kh√¥ng                      |
| `reranker_model`      | `string` |       ‚úñÔ∏é | ‚Äì             | Model reranker                                 |
| `byok_voyage_api_key` | `string` |       ‚úñÔ∏é | ‚Äì             | API key Voyage t·ª± cung c·∫•p                     |

---

## V√≠ d·ª• Request

```json
{
  "dataset_id": "ds_demo",
  "query": "H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t h·ªá th·ªëng",
  "mode": "collapsed",
  "top_k": 5,
  "expand_k": 3
}
```

---

## Response (200 OK)

```json
{
  "code": 200,
  "data": [
    {
      "chunk_id": "c123",
      "doc_id": "d456",
      "text": "ƒê·ªÉ c√†i ƒë·∫∑t h·ªá th·ªëng, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau...",
      "dist": 0.234
    }
  ]
}
```

---

### 2.3 Truy xu·∫•t v√† tr·∫£ l·ªùi ‚Äì `POST /v1/document/answer`

**Content-Type:** `application/json`

**M√¥ t·∫£:** Truy xu·∫•t th√¥ng tin li√™n quan v√† t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ LLM.

---

## Request Body

| Tr∆∞·ªùng                | Ki·ªÉu     | B·∫Øt bu·ªôc | M·∫∑c ƒë·ªãnh        | Ghi ch√∫                                        |
| --------------------- | -------- | -------: | --------------- | ---------------------------------------------- |
| `dataset_id`          | `string` |       ‚úîÔ∏é | ‚Äì               | ID b·ªô d·ªØ li·ªáu                                  |
| `query`               | `string` |       ‚úîÔ∏é | ‚Äì               | Truy v·∫•n t√¨m ki·∫øm                              |
| `mode`                | `string` |       ‚úñÔ∏é | `"collapsed"`   | `"collapsed"` ho·∫∑c `"traversal"`               |
| `top_k`               | `int`    |       ‚úñÔ∏é | `8`             | S·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn b·∫£n truy xu·∫•t                |
| `expand_k`            | `int`    |       ‚úñÔ∏é | `5`             | S·ªë l∆∞·ª£ng node m·ªü r·ªông (ch·ªâ v·ªõi mode collapsed) |
| `levels_cap`          | `int`    |       ‚úñÔ∏é | `0`             | Gi·ªõi h·∫°n c·∫•p ƒë·ªô c√¢y (ch·ªâ v·ªõi mode traversal)   |
| `use_reranker`        | `bool`   |       ‚úñÔ∏é | `false`         | C√≥ s·ª≠ d·ª•ng reranker kh√¥ng                      |
| `reranker_model`      | `string` |       ‚úñÔ∏é | ‚Äì               | Model reranker                                 |
| `byok_voyage_api_key` | `string` |       ‚úñÔ∏é | ‚Äì               | API key Voyage t·ª± cung c·∫•p                     |
| `answer_model`        | `string` |       ‚úñÔ∏é | `"DeepSeek-V3"` | Model LLM ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi                   |
| `temperature`         | `float`  |       ‚úñÔ∏é | `0.3`           | ƒê·ªô s√°ng t·∫°o c·ªßa LLM                            |
| `max_tokens`          | `int`    |       ‚úñÔ∏é | `4000`          | S·ªë token t·ªëi ƒëa trong c√¢u tr·∫£ l·ªùi              |
| `stream`              | `bool`   |       ‚úñÔ∏é | `false`         | Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo lu·ªìng (stream)             |

---

## V√≠ d·ª• Request

```json
{
  "dataset_id": "ds_demo",
  "query": "H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t h·ªá th·ªëng",
  "mode": "collapsed",
  "top_k": 5,
  "answer_model": "DeepSeek-V3",
  "temperature": 0.7,
  "max_tokens": 2000
}
```

---

## Response (200 OK)

```
{
  "answer": "ƒê·ªÉ c√†i ƒë·∫∑t h·ªá th·ªëng, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau...",
  "model": "DeepSeek-V3",
  "top_k": 5,
  "mode": "collapsed",
  "passages": [
    {
      "chunk_id": "c123",
      "doc_id": "d456",
      "text": "ƒê·ªÉ c√†i ƒë·∫∑t h·ªá th·ªëng, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau..."
    }
  ]
}
```

---

### 2.4 Qu·∫£n l√Ω Dataset ‚Äì `GET /v1/datasets`

**M√¥ t·∫£:** Li·ªát k√™ t·∫•t c·∫£ c√°c dataset/knowledge base c√≥ s·∫µn.

---

## Response (200 OK)

```json
{
  "datasets": [
    {
      "id": "ds_demo",
      "name": "ds_demo",
      "description": "Knowledge base with 5 documents",
      "document_count": 5,
      "created_at": "2023-01-01T00:00:00Z",
      "last_updated": "2023-01-02T00:00:00Z"
    }
  ],
  "total": 1
}
```

---

### 2.5 Chi ti·∫øt Dataset ‚Äì `GET /v1/datasets/{dataset_id}`

**M√¥ t·∫£:** L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ m·ªôt dataset c·ª• th·ªÉ.

---

## Response (200 OK)

```json
{
  "id": "ds_demo",
  "name": "ds_demo",
  "description": "Knowledge base containing 5 documents",
  "document_count": 5,
  "chunk_count": 120,
  "embedding_count": 120,
  "tree_count": 1,
  "created_at": "2023-01-01T00:00:00Z",
  "last_updated": "2023-01-02T00:00:00Z",
  "status": "active"
}
```

---

### 2.6 Qu·∫£n l√Ω Tr·ª£ l√Ω AI ‚Äì `POST /v1/ai/assistants`

**Content-Type:** `application/json`

**M√¥ t·∫£:** T·∫°o m·ªôt tr·ª£ l√Ω AI m·ªõi v·ªõi c·∫•u h√¨nh c·ª• th·ªÉ.

---

## Request Body

| Tr∆∞·ªùng            | Ki·ªÉu       | B·∫Øt bu·ªôc | Ghi ch√∫                                       |
| ----------------- | ---------- | -------: | --------------------------------------------- |
| `name`            | `string`   |       ‚úîÔ∏é | T√™n tr·ª£ l√Ω (3-100 k√Ω t·ª±)                      |
| `description`     | `string`   |       ‚úñÔ∏é | M√¥ t·∫£ tr·ª£ l√Ω (t·ªëi ƒëa 500 k√Ω t·ª±)               |
| `knowledge_bases` | `string[]` |       ‚úîÔ∏é | Danh s√°ch ID dataset (√≠t nh·∫•t 1)              |
| `model_settings`  | `object`   |       ‚úîÔ∏é | C·∫•u h√¨nh model                                |
| `user_id`         | `string`   |       ‚úñÔ∏é | ID ng∆∞·ªùi d√πng (cho qu·∫£n l√Ω t∆∞∆°ng lai)         |
| `system_prompt`   | `string`   |       ‚úñÔ∏é | Prompt h·ªá th·ªëng t√πy ch·ªânh (t·ªëi ƒëa 2000 k√Ω t·ª±) |

---

## Model Settings Object

| Tr∆∞·ªùng             | Ki·ªÉu     | B·∫Øt bu·ªôc | M·∫∑c ƒë·ªãnh | Ghi ch√∫                                 |
| ------------------ | -------- | -------: | -------- | --------------------------------------- |
| `model`            | `string` |       ‚úîÔ∏é | ‚Äì        | T√™n model (vd: "gpt-4o", "DeepSeek-V3") |
| `temperature`      | `float`  |       ‚úñÔ∏é | `0.7`    | ƒê·ªô s√°ng t·∫°o (0.0 - 2.0)                 |
| `topP`             | `float`  |       ‚úñÔ∏é | `1.0`    | Top P (0.0 - 1.0)                       |
| `presencePenalty`  | `float`  |       ‚úñÔ∏é | `0.0`    | Penalty s·ª± hi·ªán di·ªán (-2.0 - 2.0)       |
| `frequencyPenalty` | `float`  |       ‚úñÔ∏é | `0.0`    | Penalty t·∫ßn su·∫•t (-2.0 - 2.0)           |

---

## V√≠ d·ª• Request

```json
{
  "name": "Tr·ª£ l√Ω t√†i li·ªáu",
  "description": "Tr·ª£ l√Ω gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ t√†i li·ªáu",
  "knowledge_bases": ["ds_demo"],
  "model_settings": {
    "model": "DeepSeek-V3",
    "temperature": 0.7,
    "topP": 1.0,
    "presencePenalty": 0.0,
    "frequencyPenalty": 0.0
  }
}
```

---

## Response (200 OK)

```
{
  "code": 201,
  "data": {
    "assistant_id": "a1b2c3d4-e5f6-7890-g1h2-i3j4k5l6m7n8",
    "user_id": null,
    "name": "Tr·ª£ l√Ω t√†i li·ªáu",
    "description": "Tr·ª£ l√Ω gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ t√†i li·ªáu",
    "knowledge_bases": ["ds_demo"],
    "model_settings": {
      "model": "DeepSeek-V3",
      "temperature": 0.7,
      "topP": 1.0,
      "presencePenalty": 0.0,
      "frequencyPenalty": 0.0
    },
    "system_prompt": null,
    "status": "active",
    "meta": null,
    "created_at": "2023-01-01T00:00:00Z",
    "updated_at": "2023-01-01T00:00:00Z"
  },
  "message": "Assistant created successfully"
}
```

---

### 2.7 Chat ‚Äì `POST /v1/datasets/chat/sessions`

**Content-Type:** `application/json`

**M√¥ t·∫£:** T·∫°o m·ªôt phi√™n chat m·ªõi v·ªõi m·ªôt dataset c·ª• th·ªÉ.

---

## Request Body

| Tr∆∞·ªùng             | Ki·ªÉu     | B·∫Øt bu·ªôc | M·∫∑c ƒë·ªãnh     | Ghi ch√∫                   |
| ------------------ | -------- | -------: | ------------ | ------------------------- |
| `dataset_id`       | `string` |       ‚úîÔ∏é | ‚Äì            | ID dataset ƒë·ªÉ chat        |
| `title`            | `string` |       ‚úñÔ∏é | `"New Chat"` | Ti√™u ƒë·ªÅ phi√™n chat        |
| `user_id`          | `string` |       ‚úñÔ∏é | ‚Äì            | ID ng∆∞·ªùi d√πng             |
| `assistant_id`     | `string` |       ‚úñÔ∏é | ‚Äì            | ID tr·ª£ l√Ω AI              |
| `assistant_config` | `object` |       ‚úñÔ∏é | ‚Äì            | C·∫•u h√¨nh tr·ª£ l√Ω t√πy ch·ªânh |

---

## V√≠ d·ª• Request

```json
{
  "dataset_id": "ds_demo",
  "title": "Phi√™n chat t√†i li·ªáu",
  "assistant_config": {
    "model": "DeepSeek-V3",
    "temperature": 0.7,
    "max_tokens": 4000,
    "top_k": 5,
    "mode": "tree"
  }
}
```

---

## Response (200 OK)

```
{
  "code": 200,
  "data": {
    "session_id": "s1a2b3c4-d5e6-f7g8-h9i0-j1k2l3m4n5o6",
    "title": "Phi√™n chat t√†i li·ªáu",
    "dataset_id": "ds_demo",
    "assistant_id": null,
    "status": "active",
    "assistant_config": {
      "model": "DeepSeek-V3",
      "temperature": 0.7,
      "max_tokens": 4000,
      "top_k": 5,
      "mode": "tree"
    },
    "created_at": "2023-01-01T00:00:00Z",
    "message_count": 0
  }
}
```

---

### 2.8 G·ª≠i tin nh·∫Øn chat ‚Äì `POST /v1/datasets/chat/chat`

**Content-Type:** `application/json`

**M√¥ t·∫£:** G·ª≠i m·ªôt tin nh·∫Øn v√† nh·∫≠n ph·∫£n h·ªìi t·ª´ AI v·ªõi ng·ªØ c·∫£nh chat.

---

## Request Body

| Tr∆∞·ªùng         | Ki·ªÉu     | B·∫Øt bu·ªôc | M·∫∑c ƒë·ªãnh        | Ghi ch√∫                            |
| -------------- | -------- | -------: | --------------- | ---------------------------------- |
| `dataset_id`   | `string` |       ‚úîÔ∏é | ‚Äì               | ID dataset                         |
| `query`        | `string` |       ‚úîÔ∏é | ‚Äì               | N·ªôi dung tin nh·∫Øn                  |
| `session_id`   | `string` |       ‚úñÔ∏é | ‚Äì               | ID phi√™n chat (n·∫øu c√≥)             |
| `mode`         | `string` |       ‚úñÔ∏é | `"tree"`        | `"tree"` ho·∫∑c `"traversal"`        |
| `top_k`        | `int`    |       ‚úñÔ∏é | `8`             | S·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn b·∫£n truy xu·∫•t    |
| `expand_k`     | `int`    |       ‚úñÔ∏é | `5`             | S·ªë l∆∞·ª£ng node m·ªü r·ªông              |
| `answer_model` | `string` |       ‚úñÔ∏é | `"DeepSeek-V3"` | Model LLM ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi       |
| `temperature`  | `float`  |       ‚úñÔ∏é | `0.7`           | ƒê·ªô s√°ng t·∫°o c·ªßa LLM                |
| `max_tokens`   | `int`    |       ‚úñÔ∏é | `4000`          | S·ªë token t·ªëi ƒëa trong c√¢u tr·∫£ l·ªùi  |
| `stream`       | `bool`   |       ‚úñÔ∏é | `false`         | Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo lu·ªìng (stream) |

---

## V√≠ d·ª• Request

```json
{
  "dataset_id": "ds_demo",
  "query": "H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t h·ªá th·ªëng",
  "session_id": "s1a2b3c4-d5e6-f7g8-h9i0-j1k2l3m4n5o6",
  "mode": "tree",
  "top_k": 5,
  "answer_model": "DeepSeek-V3",
  "temperature": 0.7
}
```

---

## Response (200 OK)

```
{
  "answer": "ƒê·ªÉ c√†i ƒë·∫∑t h·ªá th·ªëng, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau...",
  "model": "DeepSeek-V3",
  "top_k": 5,
  "mode": "tree",
  "passages": [
    {
      "chunk_id": "c123",
      "doc_id": "d456",
      "text": "ƒê·ªÉ c√†i ƒë·∫∑t h·ªá th·ªëng, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau..."
    }
  ],
  "session_id": "s1a2b3c4-d5e6-f7g8-h9i0-j1k2l3m4n5o6",
  "processing_time_ms": 1250,
  "user_message": {
    "message_id": "m1a2b3c4-d5e6-f7g8-h9i0-j1k2l3m4n5o6",
    "session_id": "s1a2b3c4-d5e6-f7g8-h9i0-j1k2l3m4n5o6",
    "role": "user",
    "content": "H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t h·ªá th·ªëng",
    "created_at": "2023-01-01T00:00:00Z"
  },
  "assistant_message": {
    "message_id": "m2a3b4c5-d6e7-f8g9-h0i1-j2k3l4m5n6o7",
    "session_id": "s1a2b3c4-d5e6-f7g8-h9i0-j1k2l3m4n5o6",
    "role": "assistant",
    "content": "ƒê·ªÉ c√†i ƒë·∫∑t h·ªá th·ªëng, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau...",
    "context_passages": [
      {
        "chunk_id": "c123",
        "doc_id": "d456",
        "text": "ƒê·ªÉ c√†i ƒë·∫∑t h·ªá th·ªëng, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau..."
      }
    ],
    "model_used": "DeepSeek-V3",
    "processing_time_ms": 1250,
    "created_at": "2023-01-01T00:00:01Z"
  }
}
```

---

### 2.9 (Ngo√†i ph·∫°m vi build) Retrieve/Answer ( Ch∆∞a implement)

- `POST /v1/retrieve`: `mode=collapsed|tree_traversal`, c√≥ `reranker`.
- `POST /v1/answer`: k·∫øt h·ª£p retrieve + generate (LLM), tu·ª≥ ch·ªçn stream NDJSON/SSE.

---

## 3) Postman Collection

B·∫°n c√≥ th·ªÉ t·∫£i xu·ªëng file Postman Collection ƒë·ªÉ test c√°c API: [postman_collection.json](postman_collection.json)

ƒê·ªÉ s·ª≠ d·ª•ng collection n√†y:

1. M·ªü Postman
2. Ch·ªçn "Import" ‚Üí "Upload Files"
3. Ch·ªçn file `postman_collection.json` ƒë√£ t·∫£i xu·ªëng
4. C·∫≠p nh·∫≠t bi·∫øn `base_url` trong collection ƒë·ªÉ tr·ªè ƒë·∫øn server c·ªßa b·∫°n
5. B·∫Øt ƒë·∫ßu test c√°c API

---

## 4) üê≥ Docker Setup

### Prerequisites
- Docker Engine 20.10+
- Docker Compose 1.29+

### Development Environment

To run the application in development mode:

```bash
# Copy the example environment file
cp .env.example .env

# Edit the .env file with your configuration
# Make sure to set your database connection strings

# Start the services
docker-compose up --build

# The application will be available at:
# - Frontend: http://localhost
# - Backend API: http://localhost:8000
```

### Production Environment

For production deployment:

```bash
# Copy the example environment file
cp .env.example .env

# Edit the .env file with your production configuration

# Start the services in production mode
docker-compose -f docker-compose.prod.yml up --build -d

# The application will be available at:
# - Frontend: http://localhost
# - Backend API: http://localhost/api
```

### Docker Images

The project uses multi-stage Docker builds for both frontend and backend:

1. **Frontend**:
   - Build stage: Node.js with pnpm to build the React application
   - Production stage: Nginx to serve the static files

2. **Backend**:
   - Base stage: Python 3.11 with uv for dependency management
   - Dev stage: FastAPI development server with hot reload
   - Prod stage: Optimized FastAPI production server

### Configuration

Environment variables can be set in the `.env` file. See `.env.example` for available options.

### Volumes

The development setup includes volume mounts for hot reloading:
- Backend: Source code is mounted for live updates
- Frontend: Source code is mounted for live updates

## üß™ Testing

To run tests in Docker:

```bash
# Run backend tests
docker-compose run api pytest

# Or build and run the test stage
docker build --target test -t raptor-test .
docker run raptor-test
```

---

## 5) Stages chi ti·∫øt c·ªßa Build

### Stage 0 ‚Äì Upload gate & Persist Document

- Ch·ªâ ch·∫•p nh·∫≠n **`.md`**; n·∫øu kh√¥ng ph·∫£i `.md` ‚Üí tr·∫£ l·ªói _unsupported_.
- L∆∞u **document g·ªëc** ƒë·ªÉ l·∫•y **`doc_id`**; ƒë√¢y l√† **JOIN barrier** b·∫Øt bu·ªôc tr∆∞·ªõc khi persist c√°c chunk/embedding (m·ªçi record con s·∫Ω FK theo `doc_id`).

### Stage 1 ‚Äì Chunking & LLM refinement (in-memory)

- **Naive Markdown chunking** (v√≠ d·ª•: `size=1200`, `overlap=200`, `separators`, `keep_separator`) ƒë·ªÉ c·∫Øt kh·ªëi vƒÉn b·∫£n theo c·∫•u tr√∫c MD.
- **Refine b·∫±ng LLM** (DeepSeek-V3, `temperature=0`) ƒë·ªÉ ‚Äús·∫°ch nhi·ªÖu‚Äù, ƒëi·ªÅu ch·ªânh ranh gi·ªõi chunk theo ng·ªØ nghƒ©a; t·∫°o `chunk_texts[]` s·∫µn s√†ng persist.

### Stage 2 ‚Äì Persist chunks

- Ghi **chunks** v√†o DB (Supabase/Postgres)
- ƒê·∫£m b·∫£o **unique (`doc_id`, `idx`)** ƒë·ªÉ truy v·∫øt & c·∫≠p nh·∫≠t ·ªïn ƒë·ªãnh.

### Stage 3 ‚Äì Contextual Embedding & Persist embeddings

- T·∫°o **contextual embeddings** cho c√°c chunk (Voyage, **`dim=1024`**), sau ƒë√≥ persist v√†o b·∫£ng embeddings (ch·ªß s·ªü h·ªØu = `chunk`, `vector(1024)`).

### Stage 4 ‚Äì Rebuild decision theo `database_id`

- **Lookup** trong DB xem `database_id` ƒë√£ c√≥ tree tr∆∞·ªõc ƒë√≥ ch∆∞a.
  - **Kh√¥ng t√¨m th·∫•y (build new tree)** ‚Üí `build_set =` _current chunks + embeddings_.
  - **C√≥ (rebuild)** ‚Üí **ch·∫°y song song**:
    - Nh√°nh 1: **Xo√° tree c≈©** (trees/tree_nodes/tree_edges) theo `database_id`.
    - Nh√°nh 2: **Fetch** c√°c _previous chunks + embeddings_ theo `database_id`.
- **Join** hai nh√°nh, sau ƒë√≥ **merge previous + current ‚Üí build_set**.

### Stage 5 ‚Äì RAPTOR per-level loop (UMAP inside Clustering)

V√≤ng l·∫∑p theo t·∫ßng ch·∫°y ƒë·∫øn khi ƒë·∫°t ƒëi·ªÅu ki·ªán d·ª´ng:

1. **Cluster (UMAP inside) + GMM**
   - Gi·∫£m chi·ªÅu b·∫±ng **UMAP** ƒë·ªÉ ·ªïn ƒë·ªãnh c·∫•u tr√∫c c·ª•m r·ªìi ph√¢n c·ª•m b·∫±ng **GMM**; ch·ªçn s·ªë c·ª•m b·∫±ng **BIC** (gi·ªõi h·∫°n `min_k..max_k` theo c·∫•u h√¨nh). :contentReference[oaicite:1]{index=1}
2. **Summarize groups** b·∫±ng LLM ƒë·ªÉ t·∫°o **summary nodes** cho t·ª´ng c·ª•m.
3. **Embed summaries** (c√≥ th·ªÉ throttle) ƒë·ªÉ t·∫°o vector cho t·∫ßng k·∫ø ti·∫øp.
4. **Persist level**: ghi **tree_nodes**, **tree_edges**, c·∫≠p nh·∫≠t `current_ids/vecs/texts`, tƒÉng `level`.

### Stage 6 ‚Äì ƒêi·ªÅu ki·ªán d·ª´ng

- D·ª´ng khi **ch·ªâ c√≤n 1 node ** ·ªü t·∫ßng hi·ªán t·∫°i (_root ƒë·∫°t ƒë∆∞·ª£c_) **ho·∫∑c** ƒë·∫°t **`levels_cap`** (n·∫øu c·∫•u h√¨nh).

### Stage 7 ‚Äì Finalize & Index

- Ghi **tree cu·ªëi** (root, params, stats).
- ƒê·ªÉ ph·ª•c v·ª• retrieve/traversal sau n√†y. :contentReference[oaicite:2]{index=2}

---

## 6) M√¥ h√¨nh d·ªØ li·ªáu

```
class Document {
+doc_id: string
+dataset_id: string
+source: text
+tags: jsonb
+extra_meta: jsonb
+checksum: string
+text: text
+created_at: timestamptz
}

    class Chunk {
      +id: string
      +doc_id: string
      +idx: int
      +text: text
      +token_cnt: int
      +hash: string
      +meta: jsonb
      +created_at: timestamptz
    }

    class EmbeddingOwnerType {
      +values: "chunk | tree_node"
    }

    class Embedding {
      +id: string
      +dataset_id: string
      +owner_type: EmbeddingOwnerType
      +owner_id: string
      +model: string
      +dim: int
      +v: vector
      +meta: jsonb
      +created_at: timestamptz
    }

    class Tree {
      +tree_id: string
      +doc_id: string
      +dataset_id: string
      +params: jsonb
      +created_at: timestamptz
    }

    class NodeKind {
      +values: "leaf | summary | root"
    }

    class TreeNode {
      +node_id: string
      +tree_id: string
      +level: int
      +kind: NodeKind
      +text: text
      +meta: jsonb
      +created_at: timestamptz
    }

    class TreeEdge {
      +parent_id: string
      +child_id: string
    }

    class TreeNodeChunk {
      +node_id: string
      +chunk_id: string
      +rank: int
    }

    class ChatSession {
      +session_id: string
      +user_id: string
      +assistant_id: string
      +dataset_id: string
      +title: string
      +status: ChatSessionStatus
      +assistant_config: jsonb
      +system_prompt: text
      +meta: jsonb
      +message_count: int
      +created_at: timestamptz
      +updated_at: timestamptz
    }

    class ChatMessage {
      +message_id: string
      +session_id: string
      +role: MessageRole
      +content: text
      +context_passages: jsonb
      +retrieval_query: text
      +model_used: string
      +generation_params: jsonb
      +meta: jsonb
      +token_count: int
      +processing_time_ms: int
      +created_at: timestamptz
    }

    class ChatContext {
      +context_id: string
      +session_id: string
      +context_messages: jsonb
      +context_size_tokens: int
      +max_context_tokens: int
      +summarized_history: text
      +last_message_id: string
      +created_at: timestamptz
      +updated_at: timestamptz
    }

    class MessageRole {
      +values: "user | assistant | system"
    }

    class ChatSessionStatus {
      +values: "active | archived | deleted"
    }

    class Assistant {
      +assistant_id: string
      +user_id: string
      +name: string
      +description: text
      +knowledge_bases: jsonb
      +model_settings: jsonb
      +system_prompt: text
      +status: AssistantStatus
      +meta: jsonb
      +created_at: timestamptz
      +updated_at: timestamptz
    }

    class AssistantStatus {
      +values: "active | inactive | deleted"
    }

    %% Relationships
    Document "1" *-- "0..*" Chunk : chunks
    Document "1" *-- "0..*" Tree : trees
    Tree "1" *-- "1..*" TreeNode : nodes
    ChatSession "1" *-- "0..*" ChatMessage : messages
    ChatSession --> Assistant : assistant

    %% M:N mapping
    TreeNodeChunk --> TreeNode : node
    TreeNodeChunk --> Chunk : chunk

    %% Edges between nodes
    TreeEdge --> TreeNode : parent
    TreeEdge --> TreeNode : child

    %% Polymorphic owner (dashed deps)
    Embedding ..> Chunk : owner_type=chunk
    Embedding ..> TreeNode : owner_type=tree_node

    %% Notes (use %% comments + note syntax)
    note for Chunk "Unique: (doc_id, idx)"
    note for Embedding "Index: (dataset_id, owner_type, owner_id)\nHNSW on v (cosine)"
```

---

## 7) Model Control Protocol (MCP) Integration

### Overview

The RAPTOR service supports integration with remote LLM services through the Model Context Protocol (MCP). This allows AI assistants to interact with the RAPTOR service and access its functionality through standardized tools.

### Architecture

```
flowchart TD
    A[AI Assistant] --> B[MCP Client]
    B --> C[RAPTOR MCP Server]
    C --> D[RAPTOR Service]
    C --> E[Database]
    C --> F[LLM Services]

    subgraph AI_Ecosystem
        A
        B
    end

    subgraph RAPTOR_Service
        C
        D
        E
        F
    end
```

### Available Tools

The RAPTOR MCP server exposes the following tools:

1. **ingest_document** - Ingest documents into datasets
2. **retrieve_documents** - Retrieve relevant documents from datasets
3. **answer_question** - Answer questions using RAPTOR's knowledge
4. **list_datasets** - List available datasets
5. **create_chat_session** - Create interactive chat sessions

### SSE Endpoint

The MCP server is accessible through a Server-Sent Events (SSE) endpoint at `/mcp/sse`.

### Example Client Usage

```
import asyncio
from services.mcp.client_example import RaptorMCPClient

async def example_usage():
    async with RaptorMCPClient() as client:
        # Connect to SSE endpoint
        async for message in client.connect_sse():
            print(f"Received: {message}")

            # Call tools
            result = await client.call_tool(
                "ingest_document",
                dataset_id="ds_demo",
                file_content="Sample document content"
            )
            print(f"Result: {result}")

# Run the example
# asyncio.run(example_usage())
```

### Configuration

To enable MCP, ensure the required dependencies are installed:

```
pip install mcp[cli]
```

The MCP service is automatically initialized with the main FastAPI application.

### Extending MCP

To implement your own MCP server, refer to the example implementation in `services/mcp/example_mcp_server.py`. The MCP protocol follows the official Model Context Protocol specification.
